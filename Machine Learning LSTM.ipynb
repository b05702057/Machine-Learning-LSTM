{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "作業四(5).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r67y9UpchZ38",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "給定一個語句，判斷他有沒有惡意（負面標 1，正面標 0）\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ajS_WskRo0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path_prefix = 'drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network'\n",
        "path_prefix = './'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrAlczfM_w6",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset\n",
        "有三個檔案，分別是 training_label.txt、training_nolabel.txt、testing_data.txt\n",
        "\n",
        "- training_label.txt：有 label 的 training data（句子配上 0 or 1，+++$+++ 只是分隔符號，不要理它）\n",
        "    - e.g., 1 +++$+++ are wtf ... awww thanks !\n",
        "\n",
        "- training_nolabel.txt：沒有 label 的 training data（只有句子），用來做 semi-supervised learning\n",
        "    - ex: hates being this burnt !! ouch\n",
        "\n",
        "- testing_data.txt：你要判斷 testing data 裡面的句子是 0 or 1\n",
        "\n",
        "    >id,text\n",
        "\n",
        "    >0,my dog ate our dinner . no , seriously ... he ate it .\n",
        "\n",
        "    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n",
        "\n",
        "    >2,stupid boys .. they ' re so .. stupid !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2gwKORmuViJ",
        "colab_type": "code",
        "outputId": "27612d9b-3417-4a12-89c5-5f04d47a9680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1dPHIl8ZnfDz_fxNd2ZeBYedTat2lfxcO' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_label.txt'\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1x1rJOX_ETqnOZjdMAbEE2pqIjRNa8xcc' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_nolabel.txt'\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=16CtnQwSDCob9xmm6EdHHR7PNFNiOrQ30' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/testing_data.txt'\n",
        "\n",
        "!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n",
            "To: /content/data.zip\n",
            "45.1MB [00:00, 79.9MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: training_label.txt      \n",
            "  inflating: testing_data.txt        \n",
            "  inflating: training_nolabel.txt    \n",
            "data.zip     testing_data.txt\t training_nolabel.txt\n",
            "sample_data  training_label.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hDIokoP6464",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is for filtering the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc143hSvNGr6",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICDIhhgCY2-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils.py\n",
        "# 這個 block 用來先定義一些等等常用到的函式\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_training_data(path='training_label.txt'):\n",
        "    # 把 training 時需要的 data 讀進來\n",
        "    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines() # 所有的lines\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "        return x\n",
        "\n",
        "def load_testing_data(path='testing_data'):\n",
        "    # 把 testing 時需要的 data 讀進來\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]] # 先刪除換行符號，再用逗號轉為小句子的list，把頭尾空格去掉後用join將list中的句子合併\n",
        "        X = [sen.split(' ') for sen in X]\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    # outputs => probability (float)\n",
        "    # labels => labels\n",
        "    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為有惡意\n",
        "    outputs[outputs<0.5] = 0 # 小於 0.5 為無惡意\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYE8UYQsNIxM",
        "colab_type": "text"
      },
      "source": [
        "### Train Word to Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cgGWaF8_2S3q",
        "outputId": "3dee8017-cb83-4d4d-dff8-2265713b481b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# w2v.py\n",
        "# 這個 block 是用來訓練 word to vector 的 word embedding\n",
        "# 注意！這個 block 在訓練 word to vector 時是用 cpu，可能要花到 10 分鐘以上\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from gensim.models import word2vec\n",
        "\n",
        "def train_word2vec(x):\n",
        "    # 訓練 word to vector 的 word embedding\n",
        "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
        "    # window: 一次取幾個詞來預測中間詞\n",
        "    # min_count: 出現次數大於 x 才會被納入字典\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"loading training data ...\")\n",
        "    train_x, y = load_training_data('training_label.txt')\n",
        "    train_x_no_label = load_training_data('training_nolabel.txt')\n",
        "\n",
        "    print(\"loading testing data ...\")\n",
        "    test_x = load_testing_data('testing_data.txt')\n",
        "\n",
        "    #model = train_word2vec(train_x + train_x_no_label + test_x)\n",
        "    model = train_word2vec(train_x + test_x)\n",
        "    \n",
        "    print(\"saving model ...\")\n",
        "    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
        "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading training data ...\n",
            "loading testing data ...\n",
            "saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wHLtS0wNR6w",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGKiOitk5ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess.py\n",
        "# 這個 block 用來做 data 的預處理\n",
        "from torch import nn\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class Preprocess():\n",
        "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
        "        self.w2v_path = w2v_path\n",
        "        self.sentences = sentences\n",
        "        self.sen_len = sen_len\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "    def get_w2v_model(self):\n",
        "        # 把之前訓練好的 word to vec 模型讀進來\n",
        "        self.embedding = Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "    def add_embedding(self, word): # 新增word\n",
        "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
        "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
        "        # 因為每個句子長度要一樣，因此做padding\n",
        "        # 會遇到沒看過的字，做為unknown token\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector) # 從常態分佈取值填入vector中\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0) # cat 合併（已經轉為tensor型態，因此不再使用append）\n",
        "    def make_embedding(self, load=True):\n",
        "        print(\"Get embedding ...\")\n",
        "        # 取得訓練好的 Word2vec word embedding\n",
        "        if load:\n",
        "            print(\"loading word to vec model ...\")\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # 製作一個 word2idx 的 dictionary\n",
        "        # 製作一個 idx2word 的 list\n",
        "        # 製作一個 word2vector 的 list\n",
        "        for i, word in enumerate(self.embedding.wv.vocab): # model.wv.vocab可列出所有詞彙\n",
        "            print('get words #{}'.format(i+1), end='\\r')\n",
        "            #e.g. self.word2index['he'] = 1 \n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "            self.word2idx[word] = len(self.word2idx) # 每個字的編號\n",
        "            self.idx2word.append(word) # 每個字\n",
        "            self.embedding_matrix.append(self.embedding[word]) # [word] 可回傳該word的vector\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\n",
        "        self.add_embedding(\"<PAD>\")\n",
        "        self.add_embedding(\"<UNK>\")\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "    def pad_sequence(self, sentence):\n",
        "        # 將每個句子變成一樣的長度\n",
        "        if len(sentence) > self.sen_len:\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx[\"<PAD>\"])\n",
        "        assert len(sentence) == self.sen_len # 如果不相等則報錯\n",
        "        return sentence\n",
        "    def sentence_word2idx(self):\n",
        "        # 把句子裡面的字轉成字典裡相對應的 index\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(self.sentences): # 每個句子\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen: # 每個字\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "            # 將每個句子變成一樣的長度\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "    def labels_to_tensor(self, y):\n",
        "        # 把 labels 轉成 tensor\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJB7go5NWL0",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XketwKs4lFfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data.py\n",
        "# 實作了 dataset 所需要的 '__init__', '__getitem__', '__len__'\n",
        "# 好讓 dataloader 能使用\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class TwitterDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Expected data shape like:(data_num, data_len)\n",
        "    Data can be a list of numpy array or a list of lists\n",
        "    input data shape : (data_num, seq_len, feature_dim)\n",
        "    seq_len 表示句子長度\n",
        "    __len__ will return the number of data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.label = y\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is None: return self.data[idx]\n",
        "        return self.data[idx], self.label[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNJ8xWIMNa2r",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6RJADulIq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.py\n",
        "# 這個 block 是要拿來訓練的模型\n",
        "import torch\n",
        "from torch import nn\n",
        "class LSTM_Net(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        # 製作 embedding layer\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "        self.embedding_dim = embedding.size(1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional = True)\n",
        "        # nn.lstm()接受的數據輸入為(序列長度，batch，輸入維數)，和cnn输入的方式不太一致，使用batch_first，可以將输入變為(batch，序列長度，輸入維數)\n",
        "        self.classifier = nn.Sequential( nn.BatchNorm1d(hidden_dim * 2),\n",
        "                                         nn.Dropout(dropout),\n",
        "                                         nn.Linear(hidden_dim * 2, 1), # 把每個字產生的output轉成1維，代表句子\n",
        "                                         nn.Sigmoid())\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs, None)\n",
        "        # x 的 dimension (batch, seq_len, hidden_size)\n",
        "        # 取用 LSTM 最後一層的 hidden state（最後的seq，已經把句子從頭到尾跑完）\n",
        "        x = x[:, -1, :] \n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlpEL0sNc10",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QR4MMz-lR7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.py\n",
        "# 這個 block 是用來訓練模型的\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
        "    total = sum(p.numel() for p in model.parameters()) # numel 合計參數值\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad) # 要train的參數\n",
        "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n",
        "    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n",
        "    # 二元分類專用\n",
        "    t_batch = len(train) \n",
        "    v_batch = len(valid)   \n",
        "    # momentum = 0.9 # 想成空氣阻力或摩擦力，通常設為0.9\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    best_loss = 1000\n",
        "    for epoch in range(n_epoch):\n",
        "        # if epoch == 11:\n",
        "        #    momentum = 0.9 # 想成空氣阻力或摩擦力，通常設為0.9\n",
        "        #    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "        total_loss, total_acc = 0, 0\n",
        "        # 這段做 training\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "            labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n",
        "            outputs = model(inputs) # 將 input 餵給模型\n",
        "            outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "            # 使其與labels之維度一致\n",
        "            loss = criterion(outputs, labels) # 計算此時模型的 training loss\n",
        "            loss.backward() # 算 loss 的 gradient\n",
        "            optimizer.step() # 更新訓練模型的參數\n",
        "            correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy\n",
        "            total_acc += (correct / batch_size)\n",
        "            total_loss += loss.item()\n",
        "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
        "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
        "        print(epoch)\n",
        "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
        "\n",
        "        # 這段做 validation\n",
        "        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n",
        "        with torch.no_grad():\n",
        "            total_loss, total_acc = 0, 0\n",
        "            for i, (inputs, labels) in enumerate(valid):\n",
        "                inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "                labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "                outputs = model(inputs) # 將 input 餵給模型\n",
        "                outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "                loss = criterion(outputs, labels) # 計算此時模型的 validation loss\n",
        "                correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\n",
        "                total_acc += (correct / batch_size)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
        "            if total_acc > best_acc:\n",
        "                # or total_acc > best_acc total_loss < best_loss\n",
        "                # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
        "                best_loss = total_loss\n",
        "                best_acc = total_acc\n",
        "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
        "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
        "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
        "        print('-----------------------------------------------')\n",
        "        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF5YQrupNfCS",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X2wkdAYxHYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test.py\n",
        "# 這個 block 用來對 testing_data.txt 做預測\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def testing(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            outputs[outputs>=0.5] = 1 # 大於等於 0.5 為負面\n",
        "            outputs[outputs<0.5] = 0 # 小於 0.5 為正面\n",
        "            ret_output += outputs.int().tolist()\n",
        "    \n",
        "    return ret_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnKj0KXNeoz",
        "colab_type": "text"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztIWqCmlZof",
        "colab_type": "code",
        "outputId": "89da37d7-d966-4a2b-ead5-126c7728163a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# main.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from gensim.models import word2vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 處理好各個 data 的路徑\n",
        "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
        "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
        "\n",
        "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n",
        "\n",
        "# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n",
        "sen_len = 35 # 如果句子長度不夠，資訊會遺失（training data中最長的句子有39個字）\n",
        "fix_embedding = True # fix embedding during training （否則embedding層也會訓練）\n",
        "batch_size = 128\n",
        "epoch = 15\n",
        "lr = 0.001\n",
        "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
        "model_dir = path_prefix # model directory for checkpoint model\n",
        "\n",
        "print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "train_x_no_label = load_training_data(train_no_label)\n",
        "\n",
        "# 對 input 跟 labels 做預處理\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True) # return embedding matrix 字典 (vector of words) \n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "# 製作一個 model 的對象\n",
        "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "# 150個 LSTM cell\n",
        "model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
        "\n",
        "# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n",
        "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 取用\n",
        "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# 把 data 轉成 batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "# 開始訓練\n",
        "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #24694\n",
            "total words: 24696\n",
            "\n",
            "start training, parameter total:6657301, trainable:483301\n",
            "\n",
            "0\n",
            "\n",
            "Train | Loss:0.48491 Acc: 76.576\n",
            "Valid | Loss:0.80422 Acc: 63.336 \n",
            "saving model with acc 63.336\n",
            "-----------------------------------------------\n",
            "1\n",
            "\n",
            "Train | Loss:0.43387 Acc: 80.047\n",
            "Valid | Loss:0.50528 Acc: 74.995 \n",
            "saving model with acc 74.995\n",
            "-----------------------------------------------\n",
            "2\n",
            "\n",
            "Train | Loss:0.41828 Acc: 80.853\n",
            "Valid | Loss:0.49216 Acc: 76.816 \n",
            "saving model with acc 76.816\n",
            "-----------------------------------------------\n",
            "3\n",
            "\n",
            "Train | Loss:0.40460 Acc: 81.663\n",
            "Valid | Loss:0.41684 Acc: 80.394 \n",
            "saving model with acc 80.394\n",
            "-----------------------------------------------\n",
            "4\n",
            "\n",
            "Train | Loss:0.39369 Acc: 82.304\n",
            "Valid | Loss:0.45677 Acc: 78.394 \n",
            "-----------------------------------------------\n",
            "5\n",
            "\n",
            "Train | Loss:0.38161 Acc: 82.885\n",
            "Valid | Loss:0.43527 Acc: 79.797 \n",
            "-----------------------------------------------\n",
            "6\n",
            "\n",
            "Train | Loss:0.37042 Acc: 83.508\n",
            "Valid | Loss:0.39926 Acc: 81.753 \n",
            "saving model with acc 81.753\n",
            "-----------------------------------------------\n",
            "7\n",
            "\n",
            "Train | Loss:0.35802 Acc: 84.157\n",
            "Valid | Loss:0.42008 Acc: 80.907 \n",
            "-----------------------------------------------\n",
            "8\n",
            "\n",
            "Train | Loss:0.34206 Acc: 84.952\n",
            "Valid | Loss:0.46053 Acc: 80.071 \n",
            "-----------------------------------------------\n",
            "9\n",
            "\n",
            "Train | Loss:0.32596 Acc: 85.799\n",
            "Valid | Loss:0.42387 Acc: 81.384 \n",
            "-----------------------------------------------\n",
            "10\n",
            "\n",
            "Train | Loss:0.30494 Acc: 86.794\n",
            "Valid | Loss:0.43550 Acc: 81.021 \n",
            "-----------------------------------------------\n",
            "11\n",
            "\n",
            "Train | Loss:0.28079 Acc: 87.954\n",
            "Valid | Loss:0.46707 Acc: 80.678 \n",
            "-----------------------------------------------\n",
            "12\n",
            "\n",
            "Train | Loss:0.25689 Acc: 89.171\n",
            "Valid | Loss:0.55788 Acc: 77.836 \n",
            "-----------------------------------------------\n",
            "13\n",
            "\n",
            "Train | Loss:0.23234 Acc: 90.324\n",
            "Valid | Loss:0.56452 Acc: 78.647 \n",
            "-----------------------------------------------\n",
            "14\n",
            "\n",
            "Train | Loss:0.20634 Acc: 91.585\n",
            "Valid | Loss:0.53532 Acc: 79.887 \n",
            "-----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FiMPekdyrPQ",
        "colab_type": "code",
        "outputId": "072f9d4f-e5c9-4bdb-a57b-e99f915ff034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "training(batch_size, 100, 0.0001, model_dir, train_loader, val_loader, model, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "start training, parameter total:6657301, trainable:483301\n",
            "\n",
            "0\n",
            "\n",
            "Train | Loss:0.34398 Acc: 84.818\n",
            "Valid | Loss:0.40443 Acc: 81.529 \n",
            "saving model with acc 81.529\n",
            "-----------------------------------------------\n",
            "1\n",
            "\n",
            "Train | Loss:0.33762 Acc: 85.138\n",
            "Valid | Loss:0.40613 Acc: 81.424 \n",
            "-----------------------------------------------\n",
            "2\n",
            "\n",
            "Train | Loss:0.33328 Acc: 85.368\n",
            "Valid | Loss:0.40860 Acc: 81.554 \n",
            "saving model with acc 81.554\n",
            "-----------------------------------------------\n",
            "3\n",
            "\n",
            "Train | Loss:0.32955 Acc: 85.543\n",
            "Valid | Loss:0.41557 Acc: 81.454 \n",
            "-----------------------------------------------\n",
            "4\n",
            "\n",
            "Train | Loss:0.32572 Acc: 85.721\n",
            "Valid | Loss:0.41577 Acc: 81.414 \n",
            "-----------------------------------------------\n",
            "5\n",
            "\n",
            "Train | Loss:0.32226 Acc: 85.903\n",
            "Valid | Loss:0.41411 Acc: 81.409 \n",
            "-----------------------------------------------\n",
            "6\n",
            "\n",
            "Train | Loss:0.31827 Acc: 86.094\n",
            "Valid | Loss:0.41840 Acc: 81.414 \n",
            "-----------------------------------------------\n",
            "7\n",
            "\n",
            "Train | Loss:0.31517 Acc: 86.292\n",
            "Valid | Loss:0.42173 Acc: 81.320 \n",
            "-----------------------------------------------\n",
            "8\n",
            "\n",
            "Train | Loss:0.31137 Acc: 86.470\n",
            "Valid | Loss:0.42880 Acc: 81.270 \n",
            "-----------------------------------------------\n",
            "9\n",
            "\n",
            "Train | Loss:0.30657 Acc: 86.654\n",
            "Valid | Loss:0.43099 Acc: 81.106 \n",
            "-----------------------------------------------\n",
            "10\n",
            "\n",
            "Train | Loss:0.30312 Acc: 86.864\n",
            "Valid | Loss:0.43519 Acc: 81.091 \n",
            "-----------------------------------------------\n",
            "11\n",
            "\n",
            "Train | Loss:0.29933 Acc: 87.103\n",
            "Valid | Loss:0.43580 Acc: 81.165 \n",
            "-----------------------------------------------\n",
            "12\n",
            "\n",
            "Train | Loss:0.29444 Acc: 87.285\n",
            "Valid | Loss:0.44620 Acc: 80.678 \n",
            "-----------------------------------------------\n",
            "13\n",
            "\n",
            "Train | Loss:0.29078 Acc: 87.504\n",
            "Valid | Loss:0.44583 Acc: 80.941 \n",
            "-----------------------------------------------\n",
            "14\n",
            "\n",
            "Train | Loss:0.28593 Acc: 87.775\n",
            "Valid | Loss:0.44619 Acc: 80.932 \n",
            "-----------------------------------------------\n",
            "15\n",
            "\n",
            "Train | Loss:0.28147 Acc: 87.929\n",
            "Valid | Loss:0.45907 Acc: 80.718 \n",
            "-----------------------------------------------\n",
            "16\n",
            "\n",
            "Train | Loss:0.27709 Acc: 88.219\n",
            "Valid | Loss:0.46336 Acc: 80.653 \n",
            "-----------------------------------------------\n",
            "17\n",
            "\n",
            "Train | Loss:0.27295 Acc: 88.380\n",
            "Valid | Loss:0.46418 Acc: 80.732 \n",
            "-----------------------------------------------\n",
            "18\n",
            "\n",
            "Train | Loss:0.26837 Acc: 88.662\n",
            "Valid | Loss:0.47948 Acc: 80.608 \n",
            "-----------------------------------------------\n",
            "19\n",
            "\n",
            "Train | Loss:0.26266 Acc: 88.985\n",
            "Valid | Loss:0.47813 Acc: 80.374 \n",
            "-----------------------------------------------\n",
            "20\n",
            "\n",
            "Train | Loss:0.25775 Acc: 89.187\n",
            "Valid | Loss:0.49701 Acc: 79.956 \n",
            "-----------------------------------------------\n",
            "21\n",
            "\n",
            "Train | Loss:0.25345 Acc: 89.518\n",
            "Valid | Loss:0.49985 Acc: 80.329 \n",
            "-----------------------------------------------\n",
            "22\n",
            "\n",
            "Train | Loss:0.24791 Acc: 89.816\n",
            "Valid | Loss:0.50861 Acc: 80.384 \n",
            "-----------------------------------------------\n",
            "23\n",
            "\n",
            "Train | Loss:0.24303 Acc: 90.024\n",
            "Valid | Loss:0.51642 Acc: 80.056 \n",
            "-----------------------------------------------\n",
            "24\n",
            "\n",
            "Train | Loss:0.23784 Acc: 90.352\n",
            "Valid | Loss:0.52660 Acc: 80.006 \n",
            "-----------------------------------------------\n",
            "25\n",
            "\n",
            "Train | Loss:0.23270 Acc: 90.588\n",
            "Valid | Loss:0.53279 Acc: 79.787 \n",
            "-----------------------------------------------\n",
            "26\n",
            "\n",
            "Train | Loss:0.22708 Acc: 90.941\n",
            "Valid | Loss:0.53446 Acc: 79.857 \n",
            "-----------------------------------------------\n",
            "27\n",
            "\n",
            "Train | Loss:0.22262 Acc: 91.185\n",
            "Valid | Loss:0.54113 Acc: 79.777 \n",
            "-----------------------------------------------\n",
            "28\n",
            "\n",
            "Train | Loss:0.21748 Acc: 91.453\n",
            "Valid | Loss:0.56775 Acc: 79.449 \n",
            "-----------------------------------------------\n",
            "29\n",
            "\n",
            "Train | Loss:0.21234 Acc: 91.721\n",
            "Valid | Loss:0.57675 Acc: 79.215 \n",
            "-----------------------------------------------\n",
            "30\n",
            "\n",
            "Train | Loss:0.20670 Acc: 92.035\n",
            "Valid | Loss:0.59622 Acc: 79.389 \n",
            "-----------------------------------------------\n",
            "31\n",
            "\n",
            "Train | Loss:0.20152 Acc: 92.310\n",
            "Valid | Loss:0.59452 Acc: 79.344 \n",
            "-----------------------------------------------\n",
            "32\n",
            "\n",
            "Train | Loss:0.19595 Acc: 92.612\n",
            "Valid | Loss:0.60142 Acc: 78.712 \n",
            "-----------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdf8851f748>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 50, in wait\n",
            "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 28, in poll\n",
            "    pid, sts = os.waitpid(self.pid, flag)\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3f9b1a8f6b5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ckpt.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-1a91b6d7b92c>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(batch_size, n_epoch, lr, model_dir, train, valid, model, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 計算此時模型的 training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 算 loss 的 gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 更新訓練模型的參數\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 計算此時模型的 training accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFKHmVq0FvTb",
        "colab_type": "text"
      },
      "source": [
        "# Semi-supervising"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOU3X7x6F1PO",
        "colab_type": "code",
        "outputId": "929784d6-13c9-458f-901f-7082baa7c054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_x_no_label = load_training_data(train_no_label)\n",
        "preprocess_no_label = Preprocess(train_x_no_label , sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess_no_label.make_embedding(load=True) # return embedding matrix 字典 (vector of words) \n",
        "train_x_no_label = preprocess_no_label.sentence_word2idx()\n",
        "\n",
        "no_label_dataset = TwitterDataset(X=train_x_no_label, y=None)\n",
        "no_label_loader = torch.utils.data.DataLoader(dataset = no_label_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #24694\n",
            "total words: 24696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2-XcVNUGJ2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test.py\n",
        "# 這個 block 用來對 testing_data.txt 做預測\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model.eval()\n",
        "ret_output = []\n",
        "index = []\n",
        "answer = []\n",
        "with torch.no_grad():\n",
        "    for i, inputs in enumerate(no_label_loader):\n",
        "        inputs = inputs.to(device, dtype=torch.long)\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.squeeze()\n",
        "        outputs[outputs>=0.8] = 1 # 信心足夠者\n",
        "        outputs[outputs<=0.2] = 0 \n",
        "        for j in range(len(outputs)):\n",
        "          if outputs[j] == 1 or outputs[j] == 0:\n",
        "            index.append(128 * i + j)\n",
        "            answer.append(outputs[j])\n",
        "        ret_output += outputs.int().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cBUe19QITT-",
        "colab_type": "code",
        "outputId": "4f5fb649-f9a1-4a96-fff7-df43beadb0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(answer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "823811"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D043eqBYGK2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92dKmQRTGeNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(answer)):\n",
        "  answer[i] = int(answer[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am8fEyQFGe8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "select = []\n",
        "tlist = train_x_no_label.tolist()\n",
        "for i in index:\n",
        "  select.append(tlist[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYCQy-7Gg_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_no_label = torch.LongTensor(select)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pbXVB4oGhLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = torch.LongTensor(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKHsZiLUGj5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " X_train = torch.cat((X_train, X_train_no_label))\n",
        " y_train = torch.cat((y_train, answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O1yvP2sGkCi",
        "colab_type": "code",
        "outputId": "a7434f95-010a-435e-9b22-bd3bda7c08a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1003811, 35])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWizn-M0Gr2j",
        "colab_type": "code",
        "outputId": "4122e91f-4ef5-4c3b-ac6d-2c5e6ca3b775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "new_model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "# 150個 LSTM cell\n",
        "new_model = new_model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 取用\n",
        "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# 把 data 轉成 batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "# 開始訓練\n",
        "training(batch_size, epoch*2, 0.001, model_dir, train_loader, val_loader, new_model, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "start training, parameter total:6657301, trainable:483301\n",
            "\n",
            "0\n",
            "\n",
            "Train | Loss:0.18045 Acc: 93.469\n",
            "Valid | Loss:0.51955 Acc: 81.175 \n",
            "saving model with acc 81.175\n",
            "-----------------------------------------------\n",
            "1\n",
            "\n",
            "Train | Loss:0.12622 Acc: 96.017\n",
            "Valid | Loss:0.65082 Acc: 78.886 \n",
            "-----------------------------------------------\n",
            "2\n",
            "\n",
            "Train | Loss:0.11297 Acc: 96.495\n",
            "Valid | Loss:0.51686 Acc: 81.623 \n",
            "saving model with acc 81.623\n",
            "-----------------------------------------------\n",
            "3\n",
            "\n",
            "Train | Loss:0.10430 Acc: 96.791\n",
            "Valid | Loss:0.54084 Acc: 81.713 \n",
            "saving model with acc 81.713\n",
            "-----------------------------------------------\n",
            "4\n",
            "\n",
            "Train | Loss:0.09647 Acc: 97.032\n",
            "Valid | Loss:0.55806 Acc: 81.608 \n",
            "-----------------------------------------------\n",
            "5\n",
            "\n",
            "Train | Loss:0.08953 Acc: 97.260\n",
            "Valid | Loss:0.64781 Acc: 81.131 \n",
            "-----------------------------------------------\n",
            "6\n",
            "\n",
            "Train | Loss:0.08332 Acc: 97.441\n",
            "Valid | Loss:0.58803 Acc: 81.613 \n",
            "-----------------------------------------------\n",
            "7\n",
            "\n",
            "Train | Loss:0.07723 Acc: 97.645\n",
            "Valid | Loss:0.61455 Acc: 81.414 \n",
            "-----------------------------------------------\n",
            "8\n",
            "\n",
            "Train | Loss:0.07161 Acc: 97.830\n",
            "Valid | Loss:0.62875 Acc: 81.136 \n",
            "-----------------------------------------------\n",
            "9\n",
            "\n",
            "Train | Loss:0.06645 Acc: 97.976\n",
            "Valid | Loss:0.63765 Acc: 80.817 \n",
            "-----------------------------------------------\n",
            "10\n",
            "\n",
            "Train | Loss:0.06261 Acc: 98.105\n",
            "Valid | Loss:0.66142 Acc: 81.489 \n",
            "-----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dePGQKCsPHjq",
        "colab_type": "code",
        "outputId": "51b24949-eb30-4c5a-8c60-d0d8536a047f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "training(batch_size, epoch, 0.001, model_dir, train_loader, val_loader, new_model, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "start training, parameter total:6657301, trainable:483301\n",
            "\n",
            "0\n",
            "\n",
            "Train | Loss:0.13389 Acc: 95.404\n",
            "Valid | Loss:0.47087 Acc: 81.643 \n",
            "saving model with acc 81.643\n",
            "-----------------------------------------------\n",
            "1\n",
            "\n",
            "Train | Loss:0.11993 Acc: 95.933\n",
            "Valid | Loss:0.47456 Acc: 81.673 \n",
            "saving model with acc 81.673\n",
            "-----------------------------------------------\n",
            "2\n",
            "\n",
            "Train | Loss:0.11072 Acc: 96.241\n",
            "Valid | Loss:0.48294 Acc: 81.633 \n",
            "-----------------------------------------------\n",
            "3\n",
            "\n",
            "Train | Loss:0.10203 Acc: 96.497\n",
            "Valid | Loss:0.50528 Acc: 81.459 \n",
            "-----------------------------------------------\n",
            "4\n",
            "\n",
            "Train | Loss:0.09492 Acc: 96.743\n",
            "Valid | Loss:0.48954 Acc: 81.768 \n",
            "saving model with acc 81.768\n",
            "-----------------------------------------------\n",
            "5\n",
            "\n",
            "Train | Loss:0.08738 Acc: 97.005\n",
            "Valid | Loss:0.53290 Acc: 81.245 \n",
            "-----------------------------------------------\n",
            "6\n",
            "\n",
            "Train | Loss:0.08015 Acc: 97.264\n",
            "Valid | Loss:0.57743 Acc: 81.305 \n",
            "-----------------------------------------------\n",
            "7\n",
            "\n",
            "Train | Loss:0.07275 Acc: 97.519\n",
            "Valid | Loss:0.57333 Acc: 81.593 \n",
            "-----------------------------------------------\n",
            "8\n",
            "\n",
            "Train | Loss:0.06633 Acc: 97.748\n",
            "Valid | Loss:0.61732 Acc: 81.240 \n",
            "-----------------------------------------------\n",
            "9\n",
            "\n",
            "Train | Loss:0.06023 Acc: 97.962\n",
            "Valid | Loss:0.63931 Acc: 80.941 \n",
            "-----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQeaQNeNm3L",
        "colab_type": "text"
      },
      "source": [
        "### Predict and Write to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFvjFQopxVrt",
        "colab_type": "code",
        "outputId": "078a2b80-6ab4-4250-9b94-985c8a352898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# 開始測試模型並做預測\n",
        "print(\"loading testing data ...\")\n",
        "test_x = load_testing_data(testing_data)\n",
        "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx()\n",
        "test_dataset = TwitterDataset(X=test_x, y=None)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "outputs = testing(batch_size, test_loader, model, device)\n",
        "\n",
        "# 寫到 csv 檔案供上傳 Kaggle\n",
        "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
        "print(\"save csv ...\")\n",
        "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
        "print(\"Finish Predicting\")\n",
        "\n",
        "# 以下是使用 command line 上傳到 Kaggle 的方式\n",
        "# 需要先 pip install kaggle、Create API Token，詳細請看 https://github.com/Kaggle/kaggle-api 以及 https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n",
        "# kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n",
        "# e.g., kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading testing data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #24694\n",
            "total words: 24696\n",
            "sentence count #200000\n",
            "load model ...\n",
            "save csv ...\n",
            "Finish Predicting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwAYLRcwcr65",
        "colab_type": "code",
        "outputId": "2151d968-5dd9-4391-d1d7-f7e8aae04e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pwd\n",
        "!ls\n",
        "# check where the files are"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "ckpt.model  predict.csv  testing_data.txt    training_nolabel.txt\n",
            "data.zip    sample_data  training_label.txt  w2v_all.model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDo73ISqcyk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('predict.csv')\n",
        "# download to computer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiX0Mjxg5CKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}