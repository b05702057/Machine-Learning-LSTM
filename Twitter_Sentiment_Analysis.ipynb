{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Twitter_Sentiment_Analysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"r67y9UpchZ38"},"source":["# Recurrent Neural Networks\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"9ajS_WskRo0S"},"source":["path_prefix = './'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9YrAlczfM_w6"},"source":["### Download Dataset"]},{"cell_type":"code","metadata":{"id":"x2gwKORmuViJ","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"2c248894-01fe-483a-8f22-a9ea1afd6008"},"source":["!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n","!unzip data.zip\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n","To: /content/data.zip\n","45.1MB [00:00, 87.7MB/s]\n","Archive:  data.zip\n","  inflating: training_label.txt      \n","  inflating: testing_data.txt        \n","  inflating: training_nolabel.txt    \n","data.zip     testing_data.txt\t training_nolabel.txt\n","sample_data  training_label.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8hDIokoP6464"},"source":["# this is for filtering the warnings\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fc143hSvNGr6"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"ICDIhhgCY2-M"},"source":["# utils.py\n","# 這個 block 用來先定義一些等等常用到的函式\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def load_training_data(path='training_label.txt'):\n","    # 把 training 時需要的 data 讀進來\n","    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n","    if 'training_label' in path:\n","        with open(path, 'r') as f:\n","            lines = f.readlines() # 所有的lines\n","            lines = [line.strip('\\n').split(' ') for line in lines]\n","        x = [line[2:] for line in lines]\n","        y = [line[0] for line in lines]\n","        return x, y\n","    else:\n","        with open(path, 'r') as f:\n","            lines = f.readlines()\n","            x = [line.strip('\\n').split(' ') for line in lines]\n","        return x\n","\n","def load_testing_data(path='testing_data'):\n","    # 把 testing 時需要的 data 讀進來\n","    with open(path, 'r') as f:\n","        lines = f.readlines()\n","        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]] # 先刪除換行符號，再用逗號轉為小句子的list，把頭尾空格去掉後用join將list中的句子合併\n","        X = [sen.split(' ') for sen in X]\n","    return X\n","\n","def evaluation(outputs, labels):\n","    # outputs => probability (float)\n","    # labels => labels\n","    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為有惡意\n","    outputs[outputs<0.5] = 0 # 小於 0.5 為無惡意\n","    correct = torch.sum(torch.eq(outputs, labels)).item()\n","    return correct"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYE8UYQsNIxM"},"source":["### Train Word to Vector"]},{"cell_type":"code","metadata":{"id":"cgGWaF8_2S3q","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"cd2d7a26-9a08-41b3-b119-be530a2b9ccb"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import argparse\n","from gensim.models import word2vec\n","\n","def train_word2vec(x):\n","    # 訓練 word to vector 的 word embedding\n","    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n","    # window: 一次取幾個詞來預測中間詞\n","    # min_count: 出現次數大於 x 才會被納入字典\n","    return model\n","\n","if __name__ == \"__main__\":\n","    print(\"loading training data ...\")\n","    train_x, y = load_training_data('training_label.txt')\n","    train_x_no_label = load_training_data('training_nolabel.txt')\n","\n","    print(\"loading testing data ...\")\n","    test_x = load_testing_data('testing_data.txt')\n","\n","    #model = train_word2vec(train_x + train_x_no_label + test_x)\n","    model = train_word2vec(train_x + test_x)\n","    \n","    print(\"saving model ...\")\n","    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n","    model.save(os.path.join(path_prefix, 'w2v_all.model'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading training data ...\n","loading testing data ...\n","saving model ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3wHLtS0wNR6w"},"source":["### Data Preprocess"]},{"cell_type":"code","metadata":{"id":"CfGKiOitk5ob"},"source":["# preprocess.py\n","# 這個 block 用來做 data 的預處理\n","from torch import nn\n","from gensim.models import Word2Vec\n","\n","class Preprocess():\n","    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n","        self.w2v_path = w2v_path\n","        self.sentences = sentences\n","        self.sen_len = sen_len\n","        self.idx2word = []\n","        self.word2idx = {}\n","        self.embedding_matrix = []\n","    def get_w2v_model(self):\n","        # 把之前訓練好的 word to vec 模型讀進來\n","        self.embedding = Word2Vec.load(self.w2v_path)\n","        self.embedding_dim = self.embedding.vector_size\n","    def add_embedding(self, word): # 新增word\n","        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n","        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n","        # 因為每個句子長度要一樣，因此做padding\n","        # 會遇到沒看過的字，做為unknown token\n","        vector = torch.empty(1, self.embedding_dim)\n","        torch.nn.init.uniform_(vector) # 從常態分佈取值填入vector中\n","        self.word2idx[word] = len(self.word2idx)\n","        self.idx2word.append(word)\n","        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0) # cat 合併（已經轉為tensor型態，因此不再使用append）\n","    def make_embedding(self, load=True):\n","        print(\"Get embedding ...\")\n","        # 取得訓練好的 Word2vec word embedding\n","        if load:\n","            print(\"loading word to vec model ...\")\n","            self.get_w2v_model()\n","        else:\n","            raise NotImplementedError\n","        # 製作一個 word2idx 的 dictionary\n","        # 製作一個 idx2word 的 list\n","        # 製作一個 word2vector 的 list\n","        for i, word in enumerate(self.embedding.wv.vocab): # model.wv.vocab可列出所有詞彙\n","            print('get words #{}'.format(i+1), end='\\r')\n","            #e.g. self.word2index['he'] = 1 \n","            #e.g. self.index2word[1] = 'he'\n","            #e.g. self.vectors[1] = 'he' vector\n","            self.word2idx[word] = len(self.word2idx) # 每個字的編號\n","            self.idx2word.append(word) # 每個字\n","            self.embedding_matrix.append(self.embedding[word]) # [word] 可回傳該word的vector\n","        print('')\n","        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n","        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\n","        self.add_embedding(\"<PAD>\")\n","        self.add_embedding(\"<UNK>\")\n","        print(\"total words: {}\".format(len(self.embedding_matrix)))\n","        return self.embedding_matrix\n","    def pad_sequence(self, sentence):\n","        # 將每個句子變成一樣的長度\n","        if len(sentence) > self.sen_len:\n","            sentence = sentence[:self.sen_len]\n","        else:\n","            pad_len = self.sen_len - len(sentence)\n","            for _ in range(pad_len):\n","                sentence.append(self.word2idx[\"<PAD>\"])\n","        assert len(sentence) == self.sen_len # 如果不相等則報錯\n","        return sentence\n","    def sentence_word2idx(self):\n","        # 把句子裡面的字轉成字典裡相對應的 index\n","        sentence_list = []\n","        for i, sen in enumerate(self.sentences): # 每個句子\n","            print('sentence count #{}'.format(i+1), end='\\r')\n","            sentence_idx = []\n","            for word in sen: # 每個字\n","                if (word in self.word2idx.keys()):\n","                    sentence_idx.append(self.word2idx[word])\n","                else:\n","                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n","            # 將每個句子變成一樣的長度\n","            sentence_idx = self.pad_sequence(sentence_idx)\n","            sentence_list.append(sentence_idx)\n","        return torch.LongTensor(sentence_list)\n","    def labels_to_tensor(self, y):\n","        # 把 labels 轉成 tensor\n","        y = [int(label) for label in y]\n","        return torch.LongTensor(y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3WJB7go5NWL0"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"XketwKs4lFfB"},"source":["# data.py\n","# 實作了 dataset 所需要的 '__init__', '__getitem__', '__len__'\n","# 好讓 dataloader 能使用\n","import torch\n","from torch.utils import data\n","\n","class TwitterDataset(data.Dataset):\n","    \"\"\"\n","    Expected data shape like:(data_num, data_len)\n","    Data can be a list of numpy array or a list of lists\n","    input data shape : (data_num, seq_len, feature_dim)\n","    seq_len 表示句子長度\n","    __len__ will return the number of data\n","    \"\"\"\n","    def __init__(self, X, y):\n","        self.data = X\n","        self.label = y\n","    def __getitem__(self, idx):\n","        if self.label is None: return self.data[idx]\n","        return self.data[idx], self.label[idx]\n","    def __len__(self):\n","        return len(self.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNJ8xWIMNa2r"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"ZS6RJADulIq1"},"source":["# model.py\n","# 這個 block 是要拿來訓練的模型\n","import torch\n","from torch import nn\n","class LSTM_Net(nn.Module):\n","    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n","        super(LSTM_Net, self).__init__()\n","        # 製作 embedding layer\n","        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))\n","        self.embedding.weight = torch.nn.Parameter(embedding)\n","        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n","        self.embedding.weight.requires_grad = False if fix_embedding else True\n","        self.embedding_dim = embedding.size(1)\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional = True)\n","        # nn.lstm()接受的數據輸入為(序列長度，batch，輸入維數)，和cnn输入的方式不太一致，使用batch_first，可以將输入變為(batch，序列長度，輸入維數)\n","        self.classifier = nn.Sequential( nn.BatchNorm1d(hidden_dim * 2),\n","                                         nn.Dropout(dropout),\n","                                         nn.Linear(hidden_dim * 2, 1), # 把每個字產生的output轉成1維，代表句子\n","                                         nn.Sigmoid())\n","    def forward(self, inputs):\n","        inputs = self.embedding(inputs)\n","        x, _ = self.lstm(inputs, None)\n","        # x 的 dimension (batch, seq_len, hidden_size)\n","        # 取用 LSTM 最後一層的 hidden state（最後的seq，已經把句子從頭到尾跑完）\n","        x = x[:, -1, :] \n","        x = self.classifier(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWlpEL0sNc10"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"4QR4MMz-lR7i"},"source":["# train.py\n","# 這個 block 是用來訓練模型的\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n","    total = sum(p.numel() for p in model.parameters()) # numel 合計參數值\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad) # 要train的參數\n","    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n","    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n","    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n","    # 二元分類專用\n","    t_batch = len(train) \n","    v_batch = len(valid)   \n","    # momentum = 0.9 # 想成空氣阻力或摩擦力，通常設為0.9\n","    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n","    total_loss, total_acc, best_acc = 0, 0, 0\n","    best_loss = 1000\n","    for epoch in range(n_epoch):\n","        # if epoch == 11:\n","        #    momentum = 0.9 # 想成空氣阻力或摩擦力，通常設為0.9\n","        #    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","        total_loss, total_acc = 0, 0\n","        # 這段做 training\n","        for i, (inputs, labels) in enumerate(train):\n","            inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n","            labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n","            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n","            outputs = model(inputs) # 將 input 餵給模型\n","            outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n","            # 使其與labels之維度一致\n","            loss = criterion(outputs, labels) # 計算此時模型的 training loss\n","            loss.backward() # 算 loss 的 gradient\n","            optimizer.step() # 更新訓練模型的參數\n","            correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy\n","            total_acc += (correct / batch_size)\n","            total_loss += loss.item()\n","            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n","            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n","        print(epoch)\n","        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n","\n","        # 這段做 validation\n","        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n","        with torch.no_grad():\n","            total_loss, total_acc = 0, 0\n","            for i, (inputs, labels) in enumerate(valid):\n","                inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n","                labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n","                outputs = model(inputs) # 將 input 餵給模型\n","                outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n","                loss = criterion(outputs, labels) # 計算此時模型的 validation loss\n","                correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\n","                total_acc += (correct / batch_size)\n","                total_loss += loss.item()\n","\n","            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n","            if total_acc > best_acc:\n","                # or total_acc > best_acc total_loss < best_loss\n","                # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n","                best_loss = total_loss\n","                best_acc = total_acc\n","                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n","                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n","                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n","        print('-----------------------------------------------')\n","        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qF5YQrupNfCS"},"source":["### Test"]},{"cell_type":"code","metadata":{"id":"2X2wkdAYxHYA"},"source":["# test.py\n","# 這個 block 用來對 testing_data.txt 做預測\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def testing(batch_size, test_loader, model, device):\n","    model.eval()\n","    ret_output = []\n","    with torch.no_grad():\n","        for i, inputs in enumerate(test_loader):\n","            inputs = inputs.to(device, dtype=torch.long)\n","            outputs = model(inputs)\n","            outputs = outputs.squeeze()\n","            outputs[outputs>=0.5] = 1 # 大於等於 0.5 為負面\n","            outputs[outputs<0.5] = 0 # 小於 0.5 為正面\n","            ret_output += outputs.int().tolist()\n","    \n","    return ret_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfnKj0KXNeoz"},"source":["### Main"]},{"cell_type":"code","metadata":{"id":"EztIWqCmlZof","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"cb000fa0-67e7-44e4-bd01-bacd7b21632a"},"source":["# main.py\n","import os\n","import torch\n","import argparse\n","import numpy as np\n","from torch import nn\n","from gensim.models import word2vec\n","from sklearn.model_selection import train_test_split\n","\n","# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 處理好各個 data 的路徑\n","train_with_label = os.path.join(path_prefix, 'training_label.txt')\n","train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n","testing_data = os.path.join(path_prefix, 'testing_data.txt')\n","\n","w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n","\n","# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n","sen_len = 35 # 如果句子長度不夠，資訊會遺失（training data中最長的句子有39個字）\n","fix_embedding = True # fix embedding during training （否則embedding層也會訓練）\n","batch_size = 128\n","epoch = 15\n","lr = 0.001\n","# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n","# model directory for checkpoint model\n","\n","print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n","train_x, y = load_training_data(train_with_label)\n","train_x_no_label = load_training_data(train_no_label)\n","\n","# 對 input 跟 labels 做預處理\n","preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True) # return embedding matrix 字典 (vector of words) \n","train_x = preprocess.sentence_word2idx()\n","y = preprocess.labels_to_tensor(y)\n","\n","# 製作一個 model 的對象\n","model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n","# 150個 LSTM cell\n","model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n","\n","# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n","X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n","\n","# 把 data 做成 dataset 供 dataloader 取用\n","train_dataset = TwitterDataset(X=X_train, y=y_train)\n","val_dataset = TwitterDataset(X=X_val, y=y_val)\n","\n","# 把 data 轉成 batch of tensors\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = True,\n","                                            num_workers = 8)\n","\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 8)\n","\n","# 開始訓練\n","training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #24694\n","total words: 24696\n","\n","start training, parameter total:6657301, trainable:483301\n","\n","0\n","\n","Train | Loss:0.49425 Acc: 75.768\n","Valid | Loss:0.89588 Acc: 60.077 \n","saving model with acc 60.077\n","-----------------------------------------------\n","1\n","\n","Train | Loss:0.43763 Acc: 79.926\n","Valid | Loss:0.49049 Acc: 75.916 \n","saving model with acc 75.916\n","-----------------------------------------------\n","2\n","\n","Train | Loss:0.42366 Acc: 80.681\n","Valid | Loss:0.43950 Acc: 79.538 \n","saving model with acc 79.538\n","-----------------------------------------------\n","3\n","\n","Train | Loss:0.41081 Acc: 81.398\n","Valid | Loss:0.43068 Acc: 79.842 \n","saving model with acc 79.842\n","-----------------------------------------------\n","4\n","\n","Train | Loss:0.39876 Acc: 81.996\n","Valid | Loss:0.41525 Acc: 80.842 \n","saving model with acc 80.842\n","-----------------------------------------------\n","5\n","\n","Train | Loss:0.38930 Acc: 82.532\n","Valid | Loss:0.40889 Acc: 81.235 \n","saving model with acc 81.235\n","-----------------------------------------------\n","6\n","\n","Train | Loss:0.37726 Acc: 83.180\n","Valid | Loss:0.40872 Acc: 81.320 \n","saving model with acc 81.320\n","-----------------------------------------------\n","7\n","\n","Train | Loss:0.36506 Acc: 83.796\n","Valid | Loss:0.47814 Acc: 78.319 \n","-----------------------------------------------\n","8\n","\n","Train | Loss:0.35162 Acc: 84.448\n","Valid | Loss:0.40891 Acc: 81.220 \n","-----------------------------------------------\n","9\n","\n","Train | Loss:0.33512 Acc: 85.319\n","Valid | Loss:0.42350 Acc: 81.136 \n","-----------------------------------------------\n","10\n","\n","Train | Loss:0.31768 Acc: 86.275\n","Valid | Loss:0.43373 Acc: 80.523 \n","-----------------------------------------------\n","11\n","\n","Train | Loss:0.29614 Acc: 87.340\n","Valid | Loss:0.48735 Acc: 79.901 \n","-----------------------------------------------\n","12\n","\n","Train | Loss:0.27255 Acc: 88.501\n","Valid | Loss:0.48419 Acc: 80.643 \n","-----------------------------------------------\n","13\n","\n","Train | Loss:0.24706 Acc: 89.662\n","Valid | Loss:0.48937 Acc: 79.807 \n","-----------------------------------------------\n","14\n","\n","Train | Loss:0.22104 Acc: 90.883\n","Valid | Loss:0.56602 Acc: 80.071 \n","-----------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3yRHNDsnDnrh","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"72619ab8-fc11-41a7-cb05-e375a346c8be"},"source":["embedding.size(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["250"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"fEDCQxP5Ew7N","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"ac2e7b4c-e8f2-4f9f-b7ac-e49ff7b580d9"},"source":["embedding"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.3161,  0.2871,  0.1874,  ...,  0.3063,  0.2797, -0.5200],\n","        [ 0.3618,  0.1333, -0.2412,  ...,  0.1108, -0.0870,  0.0882],\n","        [ 0.1049, -0.2132, -0.0784,  ..., -0.0038, -0.0189, -0.0702],\n","        ...,\n","        [ 0.1167, -0.1394,  0.2990,  ...,  0.1890,  0.2447,  0.1652],\n","        [ 0.2917,  0.1253,  0.9942,  ...,  0.2789,  0.0522,  0.5639],\n","        [ 0.6998,  0.8665,  0.8057,  ...,  0.3931,  0.8750,  0.0956]])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"8FiMPekdyrPQ"},"source":["model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n","training(batch_size, 100, 0.0001, model_dir, train_loader, val_loader, model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vFKHmVq0FvTb"},"source":["# Semi-supervising"]},{"cell_type":"code","metadata":{"id":"gOU3X7x6F1PO","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"929784d6-13c9-458f-901f-7082baa7c054"},"source":["train_x_no_label = load_training_data(train_no_label)\n","preprocess_no_label = Preprocess(train_x_no_label , sen_len, w2v_path=w2v_path)\n","embedding = preprocess_no_label.make_embedding(load=True) # return embedding matrix 字典 (vector of words) \n","train_x_no_label = preprocess_no_label.sentence_word2idx()\n","\n","no_label_dataset = TwitterDataset(X=train_x_no_label, y=None)\n","no_label_loader = torch.utils.data.DataLoader(dataset = no_label_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 8)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Get embedding ...\n","loading word to vec model ...\n","get words #24694\n","total words: 24696\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c2-XcVNUGJ2s"},"source":["# test.py\n","# 這個 block 用來對 testing_data.txt 做預測\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","model.eval()\n","ret_output = []\n","index = []\n","answer = []\n","with torch.no_grad():\n","    for i, inputs in enumerate(no_label_loader):\n","        inputs = inputs.to(device, dtype=torch.long)\n","        outputs = model(inputs)\n","        outputs = outputs.squeeze()\n","        outputs[outputs>=0.8] = 1 # 信心足夠者\n","        outputs[outputs<=0.2] = 0 \n","        for j in range(len(outputs)):\n","          if outputs[j] == 1 or outputs[j] == 0:\n","            index.append(128 * i + j)\n","            answer.append(outputs[j])\n","        ret_output += outputs.int().tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cBUe19QITT-","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4f5fb649-f9a1-4a96-fff7-df43beadb0b8"},"source":["len(answer)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["823811"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"D043eqBYGK2s"},"source":["X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92dKmQRTGeNQ"},"source":["for i in range(len(answer)):\n","  answer[i] = int(answer[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Am8fEyQFGe8h"},"source":["select = []\n","tlist = train_x_no_label.tolist()\n","for i in index:\n","  select.append(tlist[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OYCQy-7Gg_V"},"source":["X_train_no_label = torch.LongTensor(select)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pbXVB4oGhLu"},"source":["answer = torch.LongTensor(answer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKHsZiLUGj5B"},"source":[" X_train = torch.cat((X_train, X_train_no_label))\n"," y_train = torch.cat((y_train, answer))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_O1yvP2sGkCi","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a7434f95-010a-435e-9b22-bd3bda7c08a4"},"source":["X_train.size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1003811, 35])"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"lWizn-M0Gr2j","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4122e91f-4ef5-4c3b-ac6d-2c5e6ca3b775"},"source":["new_model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n","# 150個 LSTM cell\n","new_model = new_model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n","\n","# 把 data 做成 dataset 供 dataloader 取用\n","train_dataset = TwitterDataset(X=X_train, y=y_train)\n","val_dataset = TwitterDataset(X=X_val, y=y_val)\n","\n","# 把 data 轉成 batch of tensors\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = True,\n","                                            num_workers = 8)\n","\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 8)\n","\n","# 開始訓練\n","training(batch_size, epoch*2, 0.001, model_dir, train_loader, val_loader, new_model, device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","start training, parameter total:6657301, trainable:483301\n","\n","0\n","\n","Train | Loss:0.18045 Acc: 93.469\n","Valid | Loss:0.51955 Acc: 81.175 \n","saving model with acc 81.175\n","-----------------------------------------------\n","1\n","\n","Train | Loss:0.12622 Acc: 96.017\n","Valid | Loss:0.65082 Acc: 78.886 \n","-----------------------------------------------\n","2\n","\n","Train | Loss:0.11297 Acc: 96.495\n","Valid | Loss:0.51686 Acc: 81.623 \n","saving model with acc 81.623\n","-----------------------------------------------\n","3\n","\n","Train | Loss:0.10430 Acc: 96.791\n","Valid | Loss:0.54084 Acc: 81.713 \n","saving model with acc 81.713\n","-----------------------------------------------\n","4\n","\n","Train | Loss:0.09647 Acc: 97.032\n","Valid | Loss:0.55806 Acc: 81.608 \n","-----------------------------------------------\n","5\n","\n","Train | Loss:0.08953 Acc: 97.260\n","Valid | Loss:0.64781 Acc: 81.131 \n","-----------------------------------------------\n","6\n","\n","Train | Loss:0.08332 Acc: 97.441\n","Valid | Loss:0.58803 Acc: 81.613 \n","-----------------------------------------------\n","7\n","\n","Train | Loss:0.07723 Acc: 97.645\n","Valid | Loss:0.61455 Acc: 81.414 \n","-----------------------------------------------\n","8\n","\n","Train | Loss:0.07161 Acc: 97.830\n","Valid | Loss:0.62875 Acc: 81.136 \n","-----------------------------------------------\n","9\n","\n","Train | Loss:0.06645 Acc: 97.976\n","Valid | Loss:0.63765 Acc: 80.817 \n","-----------------------------------------------\n","10\n","\n","Train | Loss:0.06261 Acc: 98.105\n","Valid | Loss:0.66142 Acc: 81.489 \n","-----------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dePGQKCsPHjq","colab":{"base_uri":"https://localhost:8080/","height":969},"outputId":"51b24949-eb30-4c5a-8c60-d0d8536a047f"},"source":["training(batch_size, epoch, 0.001, model_dir, train_loader, val_loader, new_model, device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","start training, parameter total:6657301, trainable:483301\n","\n","0\n","\n","Train | Loss:0.13389 Acc: 95.404\n","Valid | Loss:0.47087 Acc: 81.643 \n","saving model with acc 81.643\n","-----------------------------------------------\n","1\n","\n","Train | Loss:0.11993 Acc: 95.933\n","Valid | Loss:0.47456 Acc: 81.673 \n","saving model with acc 81.673\n","-----------------------------------------------\n","2\n","\n","Train | Loss:0.11072 Acc: 96.241\n","Valid | Loss:0.48294 Acc: 81.633 \n","-----------------------------------------------\n","3\n","\n","Train | Loss:0.10203 Acc: 96.497\n","Valid | Loss:0.50528 Acc: 81.459 \n","-----------------------------------------------\n","4\n","\n","Train | Loss:0.09492 Acc: 96.743\n","Valid | Loss:0.48954 Acc: 81.768 \n","saving model with acc 81.768\n","-----------------------------------------------\n","5\n","\n","Train | Loss:0.08738 Acc: 97.005\n","Valid | Loss:0.53290 Acc: 81.245 \n","-----------------------------------------------\n","6\n","\n","Train | Loss:0.08015 Acc: 97.264\n","Valid | Loss:0.57743 Acc: 81.305 \n","-----------------------------------------------\n","7\n","\n","Train | Loss:0.07275 Acc: 97.519\n","Valid | Loss:0.57333 Acc: 81.593 \n","-----------------------------------------------\n","8\n","\n","Train | Loss:0.06633 Acc: 97.748\n","Valid | Loss:0.61732 Acc: 81.240 \n","-----------------------------------------------\n","9\n","\n","Train | Loss:0.06023 Acc: 97.962\n","Valid | Loss:0.63931 Acc: 80.941 \n","-----------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8fQeaQNeNm3L"},"source":["### Predict and Write to csv file"]},{"cell_type":"code","metadata":{"id":"vFvjFQopxVrt","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"078a2b80-6ab4-4250-9b94-985c8a352898"},"source":["# 開始測試模型並做預測\n","print(\"loading testing data ...\")\n","test_x = load_testing_data(testing_data)\n","preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","test_x = preprocess.sentence_word2idx()\n","test_dataset = TwitterDataset(X=test_x, y=None)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 8)\n","print('\\nload model ...')\n","model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n","outputs = testing(batch_size, test_loader, model, device)\n","\n","# 寫到 csv 檔案供上傳 Kaggle\n","tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n","print(\"save csv ...\")\n","tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n","print(\"Finish Predicting\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading testing data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #24694\n","total words: 24696\n","sentence count #200000\n","load model ...\n","save csv ...\n","Finish Predicting\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RwAYLRcwcr65","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"2151d968-5dd9-4391-d1d7-f7e8aae04e92"},"source":["!pwd\n","!ls\n","# check where the files are"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","ckpt.model  predict.csv  testing_data.txt    training_nolabel.txt\n","data.zip    sample_data  training_label.txt  w2v_all.model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SDo73ISqcyk2"},"source":["from google.colab import files\n","files.download('predict.csv')\n","# download to computer"],"execution_count":null,"outputs":[]}]}